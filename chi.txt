ABSTRACT
Conversational agents promise conversational interaction but fail to deliver. Efforts often emulate functional rules from human speech, without considering key characteristics that conversation must encapsulate. Given its potential in supporting long-term human-agent relationships, it is paramount that HCI focuses efforts on delivering this promise. We aim to understand what people value in conversation and how this should manifest in agents. Findings from a series of semi-structured interviews show people make a clear dichotomy between social and functional roles of conversation, emphasising the long-term dynamics of bond and trust along with the importance of context and relationship stage in the types of conversations they have. People fundamentally questioned the need for bond and common ground in agent communication, shifting to more utilitarian definitions of conversational qualities. Drawing on these findings we discuss key challenges for conversational agent design, most notably the need to redefine the design parameters for conversational agent interaction.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).

KEYWORDS
Conversational Agents, Speech HCI, Spoken Dialogue Systems, Voice User Interface Design, Interviews. 
1. Introduction
The wide proliferation of Conversational Assistants (CAs) e.g. Siri, Google Assistant, Amazon Alexa, coupled with the pivotal role of speech as a primary interface modality in human-robot and Embodied Conversational Agent (ECA) technologies makes human-machine conversation a critical topic of study. The CA market alone is predicted to reach $9 billion by the early 2020s [https://www.businesswire.com/news/home/20180723005506/en/Global-Intelligent-Virtual-Assistant-Market-2018-2023-Market]. Currently many CAs fall short of their promised conversational potential [Reeves, 2018; Moore, 2017; McTear, et al., 2016], offering interactions that are highly constrained and task oriented in nature. To address this issue developers have largely focused on imbuing CAs with more human like qualities. These include the ability to engage in social talk [Bickmore & Cassell, 2005] and humour [Luger & Sellen, 2016], or remember details from a user’s personal history [Bickmore & Pickard, Caruso, Glough-Gorr & Heeren, 2005] along with more adaptive qualities that allow systems to tailor output predictively based on a wide array of contextual factors including, for example, previous interactions [Bickmore & Picard, 2005], environmental factors [Spyros, et al., 2014; Iqbal, et al., 2011], and a user’s emotional state [De Rosis, et al., 2003] or personality [Evans & Kortum, 2010]. The general aim is to create more natural conversational interactions with speech systems [ADELE ref; Gilmartin et al], emulating to those found in human-human conversations. 

One area where improved conversational capability is thought pivotal is in the development of long-term human-agent relationships. Although this is not necessarily desirable in all areas where conversational agents are used, it could be beneficial in others such as health and social care, workplace, education or home contexts. Currently, the conversational abilities of artificial agents are modelled on structures and rules that govern the conversational behaviours of human interactions. This approach, while important in generating conversation-like abilities, does not identify the subjective characteristics that are critical in creating a positive conversational experience, nor how users feel qualities of conversation should manifest in conversational agent interactions. Our work contributes to filling this gap by identifying: 1) what characteristics people see as important in conversation, and 2) how these vary when applied to conversations with artificial agents.  

Through semi-structured interviews, we found that people clearly identified social and functional roles of conversation, with almost universal focus on the functional when discussing agent conversations. Although participants emphasised similar characteristics that were of value in both human and machine based conversation (e.g. mutual understanding, trustworthiness, active listenership and humor) they were operationalised very differently. These topics were discussed in more functional terms when reflecting on conversing with machines. People also noted how conversational form and purpose vary with friends, strangers and acquaintances and highlight the importance of conversation in developing long term relationships. Reflecting on these attributes, people compared human-agent conversation to those with strangers or casual acquaintances. They also fundamentally questioned the need to develop relationships with agents, seeing the agent as a tool rather than a dynamic social entity. Our findings are novel in that they contribute important user led findings to the growing debate about the development of conversational ability in speech based interfaces. Based on these findings we emphasises the need to redefine the design parameters for developing conversational agents through understanding the distinct differences between people’s experiences of human-human and human-machine conversations respectively. 
2 Related Work
2.1 What is Conversation?
Spoken conversation is defined as: “any interactive spoken exchange between two or more people” [The Language of Conversation book- p.1] Human spoken conversation serves many purposes, which have been broadly classified as transactional (task-based) or social (also known as interactional) [Brown and Yule, Schneider, Slade & Eggins]. Transactional conversation pursues a practical goal, often fulfilled during the course of one interaction - a simple example is the purchase of a pizza. Both interlocutors know what the goal of the dialog is, they have different clearly-defined roles, and success is measured by the efficient exchange of information leading to the buyer getting the required pizza and the seller getting the correct money. The aim of more social conversation is not to complete a task, but to build, maintain and strengthen positive relations with one or more interlocutors [Malinowski 1923; Dunbar 1998]. Social conversation ranges from short, shallow ‘phatic’ chat and social greetings to longer interactions. Examples include dinner party talk between friends, office chat, or ‘bus stop’ conversations between strangers. 

The benefits of transactional conversation are clear in that it facilitates the successful completion of a specific task. Social conversation also serves an important purpose in conversation in that it can help develop common ground [Clark, 1996], trust and rapport between interlocutors [Cheepen, 1988]. These styles of conversation often overlap in human dialogue.

2.2 Conversational Agents
Through the popularity of CA devices like Amazon Echo and Google Home, the frequency of conversational interaction with artificial systems has grown, with interest also developing in the HCI community [Munteanu et al., Porcheron et al., 2018]. Recent emphasis in HCI has been on understanding user experiences of interactions with CAs (a.k.a., Intelligent Personal Assistants [IPAs]). This, still relatively small body of work, shows that people tend to engage in simple, limited and clearly delineated task based conversations with CAs (e.g. checking the weather or setting reminders) [Luger & Sellen, 2016]. Although these devices promise much through their implied humanness, they fall short of the reflexive and adaptive interactivity that occurs in most human-human conversational experiences [Cowan et al., 2017; Porcheron , 2017; 2018]. Instead interactions revolve around brief question/answer or user-instructions/system-confirmation dialogues [Porcheron, 2018]. Even where elements of social talk are implemented this seldom transcends the constraint of a single question and response. This is a far cry from the kind of elaborative, contextual social talk seen in human conversation [Gilmartin et al., __].  The highly functional and utilitarian nature of CA interaction has led to suggestions that ‘conversation’ is a poor description of the current interaction experience [Porcheron, 2018; McTear et al,. 2016]. This deficit has motivated a number of efforts to improve the conversational capabilities in such agents [Fang et al., 2017; Papaioannou et al., 2017]. Work has looked to imbue systems with the ability to engage in social talk (Bickmore & Cassell, 2005),  humour (Luger & Sellen, 2016), and to respond adaptively to a user’s emotional state [De Rosis, et al., 2003].

Although HCI has focused recently on commercial CAs, previous research on ECAs and social robotics have explored  how natural conversational interactions with agents may benefit the user experience. A number of these agents are being developed for health [Bickmore 2004; Bickmore Mitchell & Jack, 2010- Response to relational agent by hospital patients with depressive symptoms], elderly care [Bickmore Carusom Clough=-Gorr- Acceptance and Usability of Relational Agent Interface; Vardoulakis et al., IVA 2012; Jeong et al., 2017- RO-MAN), education [Saerbeck, 2010- CHI Expressive robots], customer service [Cassell & Bickmore 2001] and workplace [Gockley et al., Valierie- roboceptionist] contexts. ECAs like REA (Cassell & Bickmore 2001) that used small talk in a real estate context, showed that users were likely to treat interactions with it more like human-human dialogue [Bickmore & Cassell, 2005]. Microsoft’s XiaoIce, a social chatbot introduced in China and envisioned as a virtual companion uses social talk to create an emotional bond with users [Shum, He, & Li, 2018]. Similarly in social robotics, conversational interaction is integral to developing long term rapport associated with continued use [Tapus, Mataric, & Scassellati, 2007]. The accommodation of social conversations may also be important to developing trust in a social agent (Looije et al., 2010). Social conversation may not be an imperative for short term interactions commonly exemplified in current popular CA use. However, in situations where the agent needs to build trust or rapport or needs to engage in frequent long term interaction, conversational capabilities may be beneficial, if not essential. 

2.3 Research Aims
Currently, the conversational abilities of artificial agents are modelled on structures and rules that govern the conversational behaviours of human interactions [Gilmartin et al., 2017]. [Ben/Emer refs. This approach generates conversation-like abilities, but does not guarantee that the critical subjective characteristics of conversation are captured in the interaction. Our work aims to identify: 1) what characteristics people see as important in conversation, and 2) how these vary when applied to conversations with artificial agents. We achieve this by presenting the results of semi-structured interviews on people’s understandings and expectations of conversations with people and machines, and what contexts they expect speech-capable machines to be most suited to in the future.
3 Method
Our study aimed to understand what users value in conversation and what features would be required in a truly conversational agent. This was conducted through a series of semi-structured interviews. A Thematic Analysis approach (Braun and Clarke, 2006) was used to analyse and identify patterns in the interview data. The rest of this section presents details for: our sample, the study procedure, and analysis of collected data.
 
3.1 Participants
Seventeen participants (M=9, F=8) were recruited from a university community via internal email. The number of participants was chosen in line with established best practice (i.e. determining when saturation had occurred). Demographic data collected in an online questionnaire as part of the interview showed participants had a mean age of 31.1 years (SD=8.58) and constituted of students and staff across a wide range of schools. The majority were native English speakers (64.7%), with the remaining participants having near-native or high-level English speaking abilities (35.3%). Most participants rated their expertise with technology as advanced (41.2%) or intermediate (35.3%), with fewer describing themselves as experts (23.5%). The majority of participants indicated they had previously used voice-based assistants (76.5%), although of this majority 58.8% said they used them very infrequently. Participants reported interacting with Siri more than other assistants (56.3%), followed by Alexa (31.3%) and Google Assistant (31.3%). Participants were provided with a €10 honorarium in exchange for taking part.

3.2 Procedure and Data Analysis
After receiving University ethics clearance, interviews were conducted over a period of three weeks, lasting an average of 40 minutes, and audio-recorded with the participants’ consent. The interviews were semi-structured, providing us with the flexibility to adjust questioning based on responses Each interview focused on four central topics: 1) Important characteristics, purposes and experiences of conversations with different types of people e.g. friends, acquaintances, strangers. 2) Conversations with machines and their possible purposes. 3) Important and non-important characteristics for machine conversation. 4) Appropriate contexts where conversational agents could be used. The participant-led characteristics discussed in the first topic were written down by the interviewer and discussed again during the third topic. Participants were not informed or prompted to discuss conversations with machines prior to being asked by the interviewer. Following the interviews, participants completed a brief online questionnaire detailing their demographics (see 3.1). 
 
The audio recordings were transcribed and analysed using Inductive Thematic Analysis (Braun and Clarke, 2006) as it is independent of theory and offers a flexible and accessible approach to the rigorous analysis of qualitative data. The data coding was initially conducted by two researchers working independently. We began with an inductive approach but then grouped themes under the central topics of the interview guide. Once all data was coded and a set of initial themes was considered, a data session with additional researchers took place. In this session, the researchers closely reviewed the coding and preliminary themes from the data. These themes were further refined by the initial data coders. All researchers had a background in HCI: two in psychology, one in linguistics/computer science; all had experience in conducting qualitative data analyses. In the following section we present the findings from our analysis. 
4 Findings
4.1 Purposes of Conversation
Two broad and interleaving purposes for conversation were clearly shown in the data. These reflected social and transactional goals at the core of most conversation. 
4.1.1 Social Purposes 
The desire to socialise with others was a commonly discussed purpose for driving conversations. This can be for establishing, maintaining or building social bond - a connection with others - centered on relationships between people [refs]. Our participants felt conversation was imperative to not only getting to know people, but forming social groups and deepening relationships.
“You can go a bit deeper into knowing people. I would say talking and having conversation is the biggest part of knowing somebody.” [P101]
Within social talk [ref], more frivolous talk may be dynamically interspersed with more serious conversation. 
“You can have very serious conversations...but then you’re just talking absolute shite…..having the craic and there’s so much historical context for that conversation so it’s enjoyable…” [P106]
4.1.2 Functional Purposes
Juxtaposing the social nature of conversation, participants described times when conversation is more goal-oriented, allowing for people to gather information they need to complete a clearly delineated task or objective. In these, the conversation may shift or end after the speaker feels their goal has been achieved. 
“In some conversations you’re just trying to elicit information and there’s a very clear purpose….There’s a very clear short term objective to the conversation. It’s very brief, very goal-oriented. Once you achieve that you just move on.” [P109]
4.2 Attributes of (Good) Conversation
Participants described several key attributes that they value in conversation. Both the purpose of conversation and whom a conversation is with can change the importance of these attributes and the role they play. These attributes reflect findings on the properties of conversation in the linguistics literature [e.g. Clark, 1996; .
4.2.1 Mutual Understanding & Common Ground
Establishing common ground with others was often mentioned as an integral feature of good conversation. Participants stressed the importance of understanding the intent and meaning behind what other speakers are saying beyond a purely semantic or lexical level.
“Sometimes people don't get what you're saying, even though you could be using simple language and talking about simple stuff, but some people don't get it. So, it's important that both of you understand what each other is saying, and I don't mean that in terms of like a language barrier.” [P105]
“Understanding what the other person is at least trying to say. I think sometimes people jump maybe a bit too quick. A good degree of empathy [is important] because that helps you fill in the gaps between what you don’t quite understand.” [P113].
As well as providing a mutually understood focus during interaction, a knowledge of others supports people’s attempts  to reach a common understanding. 
4.2.2 Trustworthiness
Trustworthiness was also discussed as key for conversation and the development of common ground and sustainable long term relationships. 
“Trustworthy is one of the major characteristics for me. I wouldn’t really bother having a long conversation with somebody I knew wasn’t trustworthy. Having personal conversations with them would be quite disastrous.” [P103]
Having trust in a partner seems to be a gateway to open the possibility of more personal conversations. Without this trust, such conversations, which are common in long term and deep relationships, may be seen as inappropriate. 
4.2.3 Active Listening
Active listening was also an important value in conversation. Participants described that paying attention, demonstrating engagement and a willingness to participate in conversation was important in a two way interactive dialogue.
“Yeah, we know when they are not following what we are saying so I guess paying attention to somebody who is speaking is very important, at least to me.” [P103]
“Wanting to continue the conversation so wanting to know what comes next and being interested in what the other person is saying. I guess feeling that it's two way as a conversation (…) a conversation has to go both ways as opposed to just one person participating.” [P109]
Conversational partners need to demonstrate both good listenership and understanding. This can be helped by specific verbal and non-verbal feedback cues, like reciprocal action [quote for feedback needed]. 
4.2.4 Humour
Humour was also a common characteristic mentioned. Participants proposed that it scaffolds and adds more substance to discussions and remarked how humour can also be a key driver for the conversation. 
“I think that conversation should not be too serious. I should always be able to have a bit of humour”. [P108]
“I think a bit of humour is fundamental to good conversation.” [P112]
“Conversations can just be humorous as well. I find there tends to be some kind of substance underlying it otherwise it’s not engaging” [P111]
Although, humour or ‘having a laugh’ may be an important purpose of conversation, P111’s comments also highlight the  humour itself needing substance within the conversation. The comment alludes to humour’s ability to soften serious intentions or deliver substantive messages in conversation, in a way that may save face for the speaker.  
4.3 The Importance of Relationships in Conversation
Participants also discussed the differences in conversational needs depending on the types and stages of relationships they have with interlocutors. 
4.3.1 Conversing with Friends
When conversing with friends, participants relied heavily on shared experiences and a history of trust. This made participants perceive close friends as primary sources for advice seeking or getting alternate perspectives, as sounding boards to offload issues and release personal tension. 
“I have a friend who I really trust a lot and respect so I usually make sure I call her and get her opinion about the situation.” [P103]
“You feel like there is someone for you if you are in a problem and if you need a second advice then that would be the best thing because they know from maybe years together.” [P114]
“Sometimes when you’re narrating an incident, you’re just doing that to get a load off your mind.” [P103]
The type of topics covered with close friends were more personal in nature compared to other types of conversational partners (see section 4.3.2). Such conversations were built upon a shared repository of memories and experiences, that are co-constructed.
“You would have intimate conversations where you would be talking about something that is person to them… so my friend’s grandmother was sick last week so something like that that is intimate, private, discreet.” [P109]
“So your conversation leads to memories, building memories together, and that’s a huge part of having a friendship that you can reference back and say we did this together and we did that together.” [P106]
Participants also saw that with very close friends there may not be a need for constant conversation, with silence symbolising a comfort between two friends. 
“…with my very close friends, I don’t need to talk - it’s not like we talk at every given moment….” [P101]
4.3.2 Conversing with Acquaintances and Strangers
When compared to conversations with close friends, those with strangers and casual acquaintances were often more superficial and functional. Common ground is formed from shared context (e.g. the workplace) or from shared identity with the interlocutor (e.g. colleagues) and used as the anchor for conversational topics and content.
“With acquaintances it’s not as personal...it would be more about the common area where we know each other. If we are colleagues we are probably talking about a meeting we attended or common acquaintances at work.” [P103]
With acquaintances and strangers, topics are more superficial and general in nature, focusing on current events, shared contexts or the need to share information.
“I usually talk about public life, like work but on a very superficial level.” [P104]
“...I suppose we talk...like surface level stuff again like the weather...or maybe sharing a complaint like if you are waiting for a bus” [P108]. 
The content of these conversations may be more limited compared to friends as participants become concerned as to what is considered appropriate in terms of topics or conversational behaviour  in these interactions.
“...it sometimes has to be more stilted because you can’t assume everyone will find the same thing appropriate. You have to minimise the scope of the conversation.” [P111]
“...the further you get out from your inner circle [of friends] the constraints come in on what you’re going to speak about.” [P106].
Conversations with these types of partners may be driven by a perceived social norm to initiate or respond to conversational approaches. For instance, not doing so may seem impolite, particularly if initiated by another speaker. Similarly, participants may use conversation to reduce feelings of awkwardness with these partners, for example in the case of periods of silence. 
“A lot of people are uncomfortable with silence so when you get very far from the core of very close friends, that discomfort you feel with silence grows….With other people I don’t spend a lot of time with, it’s very important that the time is spent talking.” [P101]
“In terms of day-to-day, you know just meeting people on the street, to me it's social expectation...Even saying nothing is saying something.” [P111]
Engaging in conversation, even if just small talk, can help make others and oneself feel comfortable in these situations. Especially with strangers, small talk and functional or transactional dialogue were clearly the main drivers of conversation. 
4.3.3 Transition Towards Friendship
Conversation was identified as a fundamental tool used to transition towards friendship, allowing participants to ‘gauge’ one another. Being able to share vulnerabilities and personal information was seen as an important step towards developing mutual trust and bond with others.
“It’s [conversation] how you gauge a connection and a forge a pathway where you can understand what is going on with this person. I think bonds can be created with much conversation for sure.” [P106]
“I think sharing a vulnerability no matter how small or even acknowledging a sameness… That shows you that the person trusts you if they’re willing to tell you something about themselves.” [P108]
An important dimension of transition is the development of common ground, discovering shared interests and traits. This was seen to develop further through repeated and shared experiences with others.
“Only if we are open about discussing all of these things we figure out what our common interests are and if there are common interests of course then the conversation flows even better and by spending more and more time you become a friend.” [P103]
5 Purposes of Agent-based Conversation
5.1 Functional over Social Conversations 
There was a marked difference in the way that participants discussed having conversations with agents compared to conversations with other people. Conversations with agents were almost universally described in functional terms. Their status as a machine meant participants perceived a high barrier to reaching the more social and emotional connections seen in human conversation.
“I would still think of a conversational agent as a tool” [P109]
“Because it’s a machine you can’t make an emotional connection with it.” [P102]

Participants identified that the concept of conversation may need to be reconsidered or defined around different parameters in agent based interaction. Emulating human interaction is perceived as difficult, if not impossible with users questioning its desirability. 

“So probably the conversation with a machine should be characterised by different aspects...like for example the clarity of the conversation.” [P104]

“So what we see as conversation, I don’t think we can replicate it so I think there has to be new parameters for what a conversation is with a machine.” [P106]

5.2 Scenarios
When considering potential scenarios where conversational agent can be valuable, participants, same as above, primarily described functional applications such as controlling home appliances, monitoring health and scheduling appointments or dealing with daily inconveniences such as accounting and doing taxes:
“If you have something in the home that is able to guide and navigate people through their illness and teach them how to self manage better. If it’s as simple as reminding them to take their medications… telling them about hospital appointments, if they have— we live in the ITA now— if they have devices that are monitoring their health that this machine is able to take that information in— and it doesn’t have to be anything sophisticated it can be anything like temperature and activity, just stuff like that. And literally it just says “you haven’t gone outside in two days, your temperature has been 102, maybe you should just do something.” [P104] 

not nagging you but at least giving you prompts and giving you options of how you might actually conveniently do this. (...)These things should definitely be linked into all that, you know. They’re aware of your taxes as well holy God, nobody understands about taxes like. It’s all stuff you have to like find out, that like you know, that there’s this machine that’s able to take all of that admin stuff and even just organise it for you. [P106]
6. Attributes of Conversation with Agents
When asked to reflect on the attributes mentioned in human conversation, it became clear that (even similar) attributes were discussed in markedly different terms.
6.1  One Way Understanding and Personalisation over Common Ground
Participants described building common ground and mutual understanding as integral to conversation with other people. Yet, when asked to reflect on this in relation to possible conversation with agents, there was a clear difference in the role they perceived common ground would play in these interactions. Many felt displeased when considering common ground operating in a similar way to in human conversation. 

“I’d be quite upset if I thought that I had common ground with a computer.” [P108]

Instead, common ground was conceptualised as personalisation, where information would be remembered by the agent to tailor their experience and over time would serve to create an illusion of common ground between machine and user. 

“I would find it very difficult to comprehend common ground with a machine. But again if you were to personalise your machine you might have the perception of common ground. For example, I like rugby so if my machine used rugby analogies when explaining things to me I'd perceive it as having a common ground.” [P105]

“The more you interact with agents the more they learn about you so the more personalised it becomes...So machines keep learning from what you give as input to them.” [P103]

It was clear that common ground was not perceived as co-constructed in machine dialogue. Participants felt that the system should lead this process, with the user making little effort to build this with an agent. 

6.2 Functional Trustworthiness & Privacy
Trustworthiness in human conversations was linked to sharing personal information and vulnerabilities to increase social bond. In agents conversations, trustworthiness was discussed exclusively in utilitarian terms in relation to ethics, security, privacy and transparency over emotional trust.
“Trustworthy when it comes to a machine is more about the security features that are built into it.” [P103]
“I think trust definitely in regards to data...you know is this machine recording our conversation? How is that information being used? How are you using my data? Who has access? I think that’s where trust might come in.” [P108]
It is clear that, there is lack of emphasis on emotional trust in the agent. That said the conceptualisation of trust in this context may still act as a gateway to further interaction, in that issues of efficiency, reliability and security may be important for frequent long term use. 
6.3 Accurate Listening
Again, like other attributes, participants defined the attributes of an agent as a listener in more functionally terms. Participants emphasised the need to be understood clearly and quickly by a machine, ideally without the need to repeat themselves. Many of the comments focused around speech recognition performance. 
“I think the voice recognition and not having to repeat yourself would be more of the receptive side of the machine...It’s not bettering your experience if you have to sit and repeatedly ask.” [P108]
“I suppose that’s number one. I want the agent to understand what I’m saying and be able to parse it properly.” [P111]
Lacking realistic verbal and non-verbal cues means that the active listenership seen in conversation with people was perceived as difficult to achieve with agents.
6.4 Humour for Specific Purposes
While humour was seen to scaffold human conversations, participants described humour as more of a novelty feature that can help make interactions with agents more interesting. However, the novelty of humour was also described as partially successful due it lacking the ‘organic’ process of humour as seen between people.
“It [humour] makes the interaction interesting and you want to keep using these kind of devices which makes it fun for you to use it.” [P103]
“I don’t think you’re going to get humour. I think it’s hit or miss. I know you can ask Siri to tell you jokes and sometimes they do and it’s usually on the dad jokes level. It’s not actually funny the way an organic conversation would be between two humans.” [P117]
While humour was seen as a potential positive feature, at least in terms of making them interesting through a novelty value, it was rarely described as a necessary feature in the same way it was for in conversations with other people.
7. The Need for Relationships in Agent-based Conversation
7.1 Becoming  Friends with Conversational Agents
As presented earlier, our analysis showed that current human-agent conversations serve primarily functional purposes. From a relationship point of view, this is similar to transactional interactions people have with strangers or casual acquaintances. As conversation was identified as the cornerstone of becoming close friends, a key question arose as to whether and how this can be accomplished with machines. In that context, participants could not overcome the functional paradigm of the human agent conversation, and were resistant to the idea of becoming friends with a machine. Conversational agents were consistently considered as tools and assistants available to serve and accommodate people. Motivations for building a different relationship with them were questioned:
“I don't know why I would want my tool to be vulnerable, or a tool to be intimate, I think like you said it has-- like if I wanna put a nail in a table, I get a hammer. If I want to find out how to get to D107, you'd put it into whatever, Siri or something like that.” [P109]
“I dunno if you really want to sit and have a conversation that didn't require something out of the machine like the weather or turn this on or make an appointment for me or you know that kind of thing but... you're not gonna make friends with a machine so…” [P117]
Participants dismissed friendships with machines as ‘un- normal’ or were reluctant envisioning having conversations with them in the same way as they would with their friends:
I mean I don't enjoy… communicating with machines (...) when machines are like trying to become human that's just not ok...because. If you're talking to a machine to get their perspective on something, it'd be bit, un-normal [P112] 
“It's not like you can chat with a chatbot about how you feel and why your morning sucked.” [P101]
As previously when discussing attributes, conversing with agents was perceived as innately different than with people and therefore not able to accommodate friendship building:
“but it's still not the same as with a human cause the machine is gonna be like... running through algorithms to go humm what are we talking about-- keep in this range so they're not gonna like interrupt halfway through go Oh my god I forgot to tell you about this, or ooh before I forget I have to tell you about that, you know that kind-- a machine is not gonna do that[P117]
7.2 Friendship Tensions
Participants further pointed to potential conflicts that can arise if people were to be friends with machines. Building a relationship with a conversational agent was seen to require significant time and effort together with a potential shift of the nature of friendship:
“I think that if your expectation is that this is going to be not only and assistant but a like a friend or a companion of some sort then you’re gonna have to have all those things, it’s gonna have to be engaged, it’s gonna have a dialogue with you, it’s gonna have to have empathy, certainly you’re gonna have to be able to build a rapport with it. But I think it will break down there because... I think that right now… like a barrier there is that… because people value sharing experience so much… we’re going to have to really reconfigure how we think about what friendship is for us for this to work… If that’s what people would want… because it’s not like having a pet… because… I think… as it’s taken hundred of years of evolving and domesticating animals to get to that stage we see them as pets that we love and are part of the family… It will be very very interesting to see if people do start adopting… machines as companions in the home.” [P106]
The prevailing perception of a master-slave relationship, invokes another key conflict such as, ‘how do you reconcile ordering around someone who is your friend?’, or more so, ‘what would it be like if a machine was to decline your requests?
“It’s a strange thing because on one hand your sort of becoming like a friend, but on the other you can order it to do what you want …or ask it what you want, something like this. Umm, so, in that sense it would be difficult to ever really think of it as a, close, acquaintance. Because then it would have the right to say no to you. And then people throw those things out the window” [P113]
A further conflict involved the monetary value of agents for companies and concerns around how will such monetary incentives will operate in the context of a human-machine friendship.
“It depends, to my idea, the main reason behind chatbots of course, like many other technologies around machine-learning is pretty much money so there is not really like a conversation going on It's more like, people want to create chatbots because they want to have like a more flexible like customer care like maybe less expensive and stuff like that so yeah, it's like a good application of like a really nice and powerful technology but it's different from conversations itself right?” [P101]
7.3 Potential for a Human-Agent Relationship 
Despite initial concerns (and reluctance) participants saw opportunities for a human-agent relationship to be of value. Such scenarios involved people who are isolated such as elderly, or struggling with mental health issues and could benefit from a conversation:
Let’s say for example it’s a case…of anxiety. And you’re sort of going, I have to go to this work thing. But I don’t want to go. And. The machine will start telling you what the benefits are of going. Maybe it’s better for your career, maybe you’ll, get more chance…you’ll be seen. All this type of stuff [P113]
“I think that those are the only two, like task based and keeping the person company”[P112]
8 Discussion
Our study aimed to discover the features and requirements for conversational interaction with agents. Our findings show that, compared to the perceived social and functional purposes of human-human conversation, machine based conversation was conceptualised in purely functional terms. Important conversational characteristics like mutual understanding and common ground, trust and active listenership, and humor were similarly mentioned when considering integral characteristics of agent dialogue. There were, however, marked differences in how these were conceptualised in the context of human-agent dialogue. Common ground development was viewed as a one way process related to personalisation. Trust and listenership were defined in terms of system performance rather than important precursors for social bond. Whilst humor was seen as a novelty rather than an integral component of conversation. Context and the relationship with the interlocutor were major moderators in different types of human-human interactions, and human-agent interactions. The predominantly functional nature of conversations  with machines were thought similar to those held with strangers and acquaintances, whilst more social and deep topics of conversation occurred among friends. People also fundamentally questioned the desire and ability to befriend or converse with an agent in the same way as with close friends. They saw the status asymmetry (i.e. the agent as a tool for the user) as a barrier to developing bonds over the long term. That said, there were limited contexts where participants felt that there will be a need for people to develop a more social bond with a conversational agent. 

8.1 Current User Perceptions are a Barrier to Conversational Agent Interaction
Our data emphasised a fundamental mismatch in perceived status between agent and user where the agent was considered a user-controlled tool rather than a potential companion or social equal. This, in tandem with perceptions of functional ability [Moore , 2017] may restrict the types of conversations that users perceive as appropriate or possible to have with an agent at present. Critically for more social and relational purposes, our findings suggest that interpersonal aspects of conversational interaction are currently entirely absent from people’s perceptions of what CAs can and should perform. These expectations support the notion of an agent as a basic dialogue partner (Branigan et al., 2011) lacking in human like conversational  capability seen in other literature (Cowan 2017; Porcheron 2018; Luger & Sellen, 2016). Our participants strongly expressed desire to not want to build bonds with CAs, and the fact that they did not perceive them as conversationalists, may stem from this core belief that agents are poor dialogue partners that should be subservient to the user. Imbuing systems with social conversational capability was seen by users as technically difficult and lacking purpose in the user controlled tool context where they imagined agents to be used.  This type of perception may be anchored by current experiences of CAs, which are not designed to satisfy interpersonal goals. Stereotypes are a key driver of perceptions of dialogue partner ability and knowledge in human conversation (Nickerson, 1999). Stereotypical views of agent capability may act as a current barrier to embracing or utilising conversational agents for social goals.  As mentioned above, these may be flexible over time, reducing current barriers to use.  

8.2 Reframing Conversation in Agent-Based Interactions
Our findings support the view that the concept of conversation with agents needs to be reframed [Porcheron, 2018; Reeves, 2017]. It is clear that participants in this paper categorised conversation with agents as almost exclusively task-oriented and functional, echoing findings in other literature (Cowan, Luger & Selen, Porcheron 2017;2018). That said, using the term conversation to describe speech interactions with agents need not be a misnomer as conversation encapsulates both functional and social talk. Although current commercial CAs are clearly not designed to deliver social conversation, there remain contexts where, because of the need to foster a long term and personal human-agent relationship, CA dialogue may need orienting towards addressing interpersonal and social goals. Yet this may not need to emulate the form or outcomes of human conversation. Rather than simulating human conversational abilities in the hope of successful social conversation between users, our findings suggest that we need to treat human-agent interaction as a new genre of conversation, with its own rules, norms and expectations. As articulated in our data these may be more functional and utilitarian in nature, with little emphasis on the relational growth outcomes or emotional outcomes seen in human conversation. Our data suggest that conversational content and structure of those we have with strangers or acquaintances may be a good starting metaphor for social agent conversations. These conversational norms and expectations will likely be dynamic, shifting as long term agent use becomes more commonplace in contexts where they are designed to address social needs.  account. 

8.3 Conversational Interaction may be Appropriate in some Contexts
Our participants questioned the desire to become friends with a system, yet identified a number of contexts where conversational capabilities could facilitate CA interaction. These often focused on contexts that had strong requirements for interpersonal relationships and social bond like health and mental health were stringer social bonds might facilitate a degree of intimacy appropriate for encouraging greater self-disclosure [Bickmore & Cassel, 2001], or elderly care scenarios where conversational capabilities could be used to counteract loneliness. Indeed, embodied conversational agents and social robots that hold various social conversational capabilities like small talk, storytelling, and humour, have been successfully developed in these domains [e.g. Sillice et al., 2018- other refs from intro]. Even though our work questions the extent to which these attributes will lead to similar benefits and relationships that accrue in human communication, that is not to say that, in some contexts, conversational capability may not facilitate interaction and use. Like in examples in social robotics [insert ref from intro], elderly users benefited from social capabilities in a system, leading them to share their stories with a system. What is integral to this is a sensitivity to the context of interaction, an understanding of the type of conversation required, and the purposes behind the interaction. Indeed, CA designers may be best placed to focus on ways to deliver the benefits of conversation through other means, especially in more embodied systems. This can be exampled by the development of social robots like Paro [Kidd, Taggart & Turkle, 2006-A social robot to encourage social interaction amongst the elderly], whereby benefits of socialisation are not accrued through spoken conversation but through using other sensory modalities. In such systems, speech could be used for tasks which match the functional expectations of users for agent based speech interaction. If designing speech only CAs, our work suggests that, based on current user expectations, conversational interactions with strangers may prove an appropriate conversational model to work from, providing guidance on social boundaries, tone and speech content [Clark, 2018]. Designing an interface that interacts  primarily, if not exclusively, with speech, may also have different design requirements to interfaces that incorporate embodied and nonverbal features. Users’ interactions and experiences with speech can be altered when embodied features such as gesture and emotive facial expressions are included [Breazeal, 2003; Bruce, Nourbakhsh, & Simmons, 2002; Kuno et al., 2007]. This may also have consequences for interactions with conversational agents.

8.5 Limitations and Future Work
This paper identifies common characteristics people view as important in conversation and how they vary when applied to conversations with artificial agents. Consequently, when considering the design of conversational agents, we may need to shift and redevelop existing design parameters for these agents. While participants stressed the unimportance of interpersonal relationships when engaging with conversational agents, this may be a consequence of a lack of familiarity with agents that are designed to engage in conversation, rather than VUIs which are not necessarily considered examples of conversational interaction [Porcheron et al., 2018]. Conversation is not necessary in all types of agents. However, people’s experience of agents are primarily brief and non-sequential question-answer pairs. Future research may need to explore attitudes towards truly conversational agents in longer, sequential interactions, as well as agents in contexts that stress interpersonal and social goals as a fundamental purpose of the system (e.g. healthcare and wellbeing). Envisioning [Reeves] and agents in the home [Pantidi].

9 Conclusion
Conversation as a means of establishing and building upon relationships is not seen as desirable with machines (according to interviews)
Relationships a big driver of conversation 
Relationships with machines...not so much an important feature/desirable consideration
If not a necessary design guidelines to have truly conversational agents, then systems can be inspired but not necessarily driven by human conversation (e.g. specific conversational elements, don’t have to map equally), 
Inspiration vs. mimicry in agent design; imperfections; believable interactions - intersubjectivity 
Mixed success of a social and adaptive robot in education (Kennedy et al, 2015). 
Many aspects of social interaction that people don;t want to see in robot (e.g. social control or criticism- Baron, 2015- Shall we talk? Conversing with Humans and robots)
